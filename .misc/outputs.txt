
*************************************************************************
****************** Part 5: Testing the layers ***************************
*************************************************************************
Input Layer
[[-1. -1. -1. -1.]
 [ 1.  1.  1.  1.]]

Fully Connected Layer
[[4.78632519e-05 4.38634319e-04]
 [5.81183644e-05 1.07682605e-03]]

ReLu Activation Layer
[[1 2 3 4]
 [5 6 7 8]]

Sigmoid Activation Layer
[[0.73105858 0.88079708 0.95257413 0.98201379]
 [0.99330715 0.99752738 0.99908895 0.99966465]]

SoftMax Activation Layer
[[0.00449655 0.00449655 0.00449655 0.00449655]
 [0.24550345 0.24550345 0.24550345 0.24550345]]

Tanh Activation Layer
[[0.76159416 0.96402758 0.99505475 0.9993293 ]
 [0.9999092  0.99998771 0.99999834 0.99999977]]

*************************************************************************
*********** Part 6: Connecting Layers and Foward Propagate **************
*************************************************************************
Input Layer
[[-1. -1. -1. -1.]
 [ 1.  1.  1.  1.]]

Fully Connected Layer
[[ 9.01687740e-05 -1.82859629e-04]
 [ 9.52963302e-05  1.36236236e-04]]

Sigmoid Activation Layer
[[0.50002254 0.49995429]
 [0.50002382 0.50003406]]

*************************************************************************
***************** Part 7: Testing on full dataset ***********************
*************************************************************************

Note: For this part I was unsure what you wanted for the output so I just 
ran the same test we did for part 6 on the actual data.


Input Layer
[[-1.43876426 -1.0105187  -0.45332    -0.90861367  1.97058663  1.34390459]
 [-1.50996545  0.98959079  0.5096211  -0.07876719 -0.5074631   0.43849455]
 [-0.79795355  0.98959079  0.38330685  1.58092576 -0.5074631   0.43849455]
 ...
 [-1.50996545 -1.0105187   1.0148781  -0.90861367 -0.5074631   0.43849455]
 [-1.29636188 -1.0105187  -0.79781341 -0.90861367 -0.5074631   1.34390459]
 [ 1.55168573 -1.0105187  -0.26138796 -0.90861367  1.97058663 -0.46691549]]

Fully Connected Layer
[[ 2.58205060e-04 -1.08465804e-04]
 [-9.06627986e-06  5.20790733e-05]
 [-2.09036365e-05  2.09081428e-04]
 ...
 [-4.75300635e-05 -1.61546673e-05]
 [ 3.50594995e-05 -5.46214776e-05]
 [ 1.78816999e-04  1.53724547e-05]]

Sigmoid Activation Layer
[[0.50006455 0.49997288]
 [0.49999773 0.50001302]
 [0.49999477 0.50005227]
 ...
 [0.49998812 0.49999596]
 [0.50000876 0.49998634]
 [0.5000447  0.50000384]]